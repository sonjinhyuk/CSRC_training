{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MalRNN 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 필요한 package 설치 확인 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 추출된 Stream data 에서의 시퀀스 추출 과정 구축\n",
    "* 추출된 stream data(.csv)에서 필요한 byte stream data 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data_path, chunk_len, malconv_min_len, len_limit=100000):\n",
    "    \"\"\"\n",
    "    :param str data_path: stream data csv파일의 위치\n",
    "    :param int chunk_len: benign byte stream sampling 길이(정상데이터는 최소 chunk_len 이상이어야 함)\n",
    "    :param int malconv_min_len: malconv 탐지를 위한 최소 길이(악성데이터는 최소 malconv_min_len 이상이어야 함)\n",
    "    :param int len_limit: 시스템 과부하 방지 목적 길이 제한\n",
    "    :return 정상/악성 byte stream data list\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ### Byte Stream 데이터 추출을 위한 함수 만들기 ###\n",
    "    # 1. data_path에 위치한 파일 열기\n",
    "    # 2. 정상/악성 데이터를 저장할 list 생성\n",
    "    # 3. len_limit 이내의 데이터에서 추출\n",
    "    # 4. csv 내의 Stream 이후만 추출하기 위한 \"]\"의 위치 찾은후 이후 데이터만 추출\n",
    "    # 5. 추출된 데이터의 개행문자 제거 및 전처리\n",
    "    # 6. 전처리된 데이터의 int 변환 및 list 삽입\n",
    "    # 7. 정상/악성 여부에 따른 최소 길이 확인 및 list 삽입\n",
    "    \n",
    "    with open(data_path, \"r\", encoding=\"cp949\") as data:\n",
    "        benign_data = []\n",
    "        critical_data = []\n",
    "\n",
    "        for line in data.readlines():\n",
    "            try:\n",
    "                if len(line) < len_limit:\n",
    "                    look = line.rfind(\"]\")\n",
    "                    line = line[look+2 : ]\n",
    "                    line = line.replace(\"\\n\", \"\")\n",
    "                    if line.startswith(\",\"):\n",
    "                        line = line[1:]\n",
    "                    line = line.split(\",\")\n",
    "                    line = [int(x,0) for x in line]\n",
    "\n",
    "                    if line[1] == 0 and len(line[4:]) > chunk_len:\n",
    "                        benign_data.append(line[4:])\n",
    "                    elif line[1] == 1 and len(line[4:]) > malconv_min_len:\n",
    "                        critical_data.append(line[4:])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return benign_data, critical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 생성된 byte stream 악성 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_detection(malconv, gen_bytes):\n",
    "    \"\"\"\n",
    "    :param nn.Module malconv: malconv 모델\n",
    "    :param list gen_bytes: 생성된 byte stream \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        gen_bytes = torch.from_numpy(np.frombuffer(gen_bytes, dtype=np.uint8)[np.newaxis, :])\n",
    "        malconv_output = F.softmax(malconv(gen_bytes), dim=-1).detach().numpy()[0,1]\n",
    "        return malconv_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 정상 데이터에 대한 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_benign_sample(benign_stream, chunk_len, batch_size, device):\n",
    "    \"\"\"\n",
    "    :param list benign_stream: 정상 byte stream\n",
    "    :param int chunk_len: sampling 길이\n",
    "    :param int batch_size: 한번 학습에 이용할 byte 갯수\n",
    "    :param device: 학습 device(CPU/GPU)에 따른 할당\n",
    "    :return input_stream, target_stream\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ### 정상 데이터 sampling 만들기 ###\n",
    "    # 1. input_stream(학습에 사용), target_stream(loss 산정 시 사용) 선언 (batch_size * chunk_len)\n",
    "    # 2. batch size 만큼의 for loop 생성\n",
    "    # 3. chunk_len을 고려한 임의의 start index 선정\n",
    "    # 4. chunk_len을 고려한 end_index 계산\n",
    "    # 5. benign stream data에서 sampling\n",
    "    # 6. input_stream과 target_stream 저장\n",
    "    # 7. 저장된 input_stream, target_stream device 할당\n",
    "    \n",
    "    input_stream = torch.LongTensor(batch_size, chunk_len)\n",
    "    target_stream = torch.LongTensor(batch_size, chunk_len)\n",
    "    for batch in range(batch_size):\n",
    "        start_index = random.randrange(0, len(benign_stream) - chunk_len)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = benign_stream[start_index : end_index]\n",
    "        input_stream[batch] = torch.as_tensor(chunk[:-1])\n",
    "        target_stream[batch] = torch.as_tensor(chunk[1:])\n",
    "    input_stream = torch.LongTensor(input_stream).to(device)\n",
    "    target_stream = torch.LongTensor(target_stream).to(device)\n",
    "    return input_stream, target_stream   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습과정에 필요한 Byte Stream 생성 함수 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_byte(model, base_stream, device, len_to_predict=1000, temperature=0.8):\n",
    "    \"\"\"\n",
    "    :param nn.Module model: Byte Stream 생성할 MalRNN 모델\n",
    "    :param list base_stream: 생성에 이용될 기초 byte stream\n",
    "    :param device: 학습 device(CPU/GPU)에 따른 할당\n",
    "    :param int len_to_predict: 생성할 Byte Stream 갯수\n",
    "    :param float temperature: 분포 smoothing을 위한 지수\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ### MalRNN을 이용해 byte stream 생성 ###\n",
    "    # 1. 입력된 base stream 적용을 위한 model의 hidden state 초기화\n",
    "    # 2. base stream을 학습에 적합하도록 unsqueeze를 이용한 차원 추가\n",
    "    # 3. 예측 variable 선언 및 base_stream 적용\n",
    "    # 4. 마지막 byte stream을 제외한 byte stream model hidden state에 적용\n",
    "    # 5. len_to_predict 길이까지 하나씩 byte stream 생성\n",
    "\n",
    "    hidden_state = model.init_hidden(1).to(device)\n",
    "    base_input = torch.LongTensor(base_stream).unsqueeze(0).to(device)\n",
    "    predict = base_stream\n",
    "\n",
    "    for p in range(len(base_stream) - 1):\n",
    "        _, hidden_state = model(base_input[:, p], hidden_state)\n",
    "\n",
    "    output_result = []\n",
    "    model_input = base_input[:, -1]\n",
    "    for p in range(len_to_predict):\n",
    "        output, hidden_state = model(model_input, hidden_state)\n",
    "        output_result.append(output)\n",
    "\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        predict_stream = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        predict = np.append(predict, predict_stream.detach().cpu())\n",
    "        model_input = (\n",
    "            torch.tensor(predict_stream, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        )\n",
    "\n",
    "    return predict.tolist(), output_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MalRNN 학습과정 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습에 필요한 moudle 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_model import CharRNN\n",
    "from MalConv import MalConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MalRNN에 필요한 파일 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_data_path = \"\"\n",
    "malconv_weight_path = \"./malconv_doc.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MalRNN():\n",
    "    \n",
    "    ### MalRNN 학습과정 구축 ###\n",
    "\n",
    "    # TODO: 정상/악성 byte stream 호출\n",
    "    benign_data, critical_data = parse_data(stream_data_path, chunk_len=200, malconv_min_len=512)\n",
    "\n",
    "    # 학습장치(CPU/GPU) 호출\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(f\"Current device is {device}\")\n",
    "\n",
    "    # MalConv 로드\n",
    "    malconv = MalConv(channels=256, window_size=512, embd_size=8)\n",
    "    malconv_weight = torch.load(malconv_weight_path)\n",
    "    malconv.load_state_dict(malconv_weight)\n",
    "\n",
    "    # TODO: MalRNN 모델 구성하기\n",
    "    model = CharRNN(\n",
    "        input_size=256,\n",
    "        hidden_size=100,\n",
    "        output_size=256,\n",
    "        model=\"gru\",\n",
    "        n_layers=1,\n",
    "    )\n",
    "\n",
    "    # MalRNN 모델 학습장치 할당\n",
    "    model.to(device)\n",
    "\n",
    "    # TODO: 학습 loss function 및 optimizer 호출\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # 학습에 필요한 variable 선언\n",
    "    loss_record = []\n",
    "    best_score = -1\n",
    "    best_loss = -1\n",
    "    best_model = None  \n",
    "    \n",
    "    # TODO 매 epoch별 학습 구성\n",
    "    # 1. 정상 stream data sampling -> input / target benign stream data 수집\n",
    "    # 2. 모델 hidden state 초기화 및 학습장치 할당\n",
    "    # 3. 모델 parameter 초기화 및 loss variable 선언\n",
    "    # 4. 악성 stream data 추출 및 생성된 stream으로 악성 회피여부 확인\n",
    "    # 5. 길이만큼 매 byte stream 학습 및 다음 byte stream 생성하여 loss 계산\n",
    "    # 6. loss에 따른 MalRNN model parameter 조정\n",
    "    # 7. loss 및 탐지 확률에 따른 모델 저장\n",
    "    \n",
    "    for epoch in range(1, 101):\n",
    "        print(f\"EPOCH {epoch}\")\n",
    "\n",
    "        input_benign, target_benign = create_benign_sample(\n",
    "            benign_stream=benign_data[random.randrange(0, len(benign_data))],\n",
    "            chunk_len=200,\n",
    "            batch_size=10,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        hidden_state = model.init_hidden(10)\n",
    "        hidden_state.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss = 0\n",
    "        base_stream = critical_data.pop(random.randrange(0, len(critical_data)))[:1024]\n",
    "        predicted, _ = generate_byte(model=model, base_stream=base_stream, device=device)\n",
    "        candidate = bytearray(base_stream) + bytearray(predicted[0])\n",
    "        malconv_result = eval_detection(malconv, candidate)\n",
    "        for c in range(200):\n",
    "            output, hidden_state = model(input_benign[:, c], hidden_state.to(device))\n",
    "            loss += criterion(output.view(10, -1), target_benign[:, c])\n",
    "        loss_record.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch} loss: {loss.data / 200}\")\n",
    "        print(f\"Detection Possibility: {malconv_result : 4f}\")\n",
    "\n",
    "        if epoch == 1:\n",
    "            print(\"Saving the first model\")\n",
    "            best_model = model\n",
    "            best_score = malconv_result\n",
    "            best_loss = loss.data / 200\n",
    "        elif best_score > malconv_result:\n",
    "            print(\"Best score updated! Saving...\")\n",
    "            best_model = model\n",
    "            best_score = malconv_result\n",
    "            best_loss = loss.data / 200\n",
    "        elif best_score == malconv_result:\n",
    "            if best_loss > (loss.data / 200):\n",
    "                print(\"Best score updated! Saving...\")\n",
    "                best_model = model\n",
    "                best_score = malconv_result\n",
    "                best_loss = loss.data / 200\n",
    "    \n",
    "    # 학습된 최종 모델 저장\n",
    "    torch.save(best_model, \"./malRNN_doc.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
