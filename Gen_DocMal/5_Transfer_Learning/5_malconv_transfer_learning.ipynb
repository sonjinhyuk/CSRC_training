{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MalConv 전이학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 필요한 package 설치 확인 및 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from MalConv import MalConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 학습을 위한 도구 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stream data csv 추출 도구 ###\n",
    "def parse_data(data_path):\n",
    "    print(\"### START PARSING DATA ###\")\n",
    "    with open(data_path, \"r\", encoding=\"cp949\") as data:\n",
    "        label = []\n",
    "        stream_data = []\n",
    "        for idx, line in enumerate(data.readlines()):\n",
    "            try:\n",
    "                if idx != 0 and len(line) < 100000:\n",
    "                    look = line.rfind(\"]\")\n",
    "                    line = line[look + 2 :]\n",
    "                    line = line.replace(\"\\n\", \"\")\n",
    "                    if line.startswith(\",\"):\n",
    "                        line = line[1:]\n",
    "                    line = line.split(\",\")\n",
    "                    line = [int(x, 0) for x in line]\n",
    "\n",
    "                    if line[1] == 0:\n",
    "                        label.append(0)\n",
    "                    else:\n",
    "                        label.append(1)\n",
    "                    stream_data.append(line[4:])\n",
    "            except:\n",
    "                pass\n",
    "        data.close()\n",
    "    print(\"### PARSING DONE! ###\")\n",
    "    return stream_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습을 위한 Dataset Class 구축 ###\n",
    "\n",
    "class StreamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Stream data를 학습에 이용할 수 있도록 Dataset class 제작\n",
    "    \"\"\"\n",
    "    def __init__(self, stream_list, label):\n",
    "        \"\"\"\n",
    "        :param stream_list: csv에서 추출한 stream list\n",
    "        :param label: 악성/정상 여부\n",
    "        \"\"\"\n",
    "        # 1. 기존 Dataset 상속\n",
    "        # 2. stream_list, label 변수 지정\n",
    "        self.stream_list = stream_list\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.stream_list[index], self.label[index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터셋 내 일정한 길이의 stream 으로 만들기 위한 함수 ###\n",
    "\n",
    "def collate_fn(batch, max_len=100000):\n",
    "    \"\"\"\n",
    "    :param int batch: batch로 구성되어 있는 데이터셋\n",
    "    :param int max_len: 학습에 이용할 stream의 최대 길이\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # 1. stream_list, label_list 변수 선언\n",
    "    # 2. batch 내의 stream, label 학습에 맞는 변형\n",
    "    # 3. max_len 초과의 길이의 경우 max_len 만큼만 자르기\n",
    "    # 4. max_len 미만의 길이의 경우 padding(0) 추가하여 길이 맞추기\n",
    "    # 5. stream은 long 형태, label은 int64 형태로 list에 삽입\n",
    "    # 6. batch에 맞는 stack 맞춰서 return\n",
    "\n",
    "    stream_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for stream, label in batch:\n",
    "        if len(stream) < max_len:\n",
    "            stream += [0] * (max_len - len(stream))\n",
    "        stream_processed = torch.tensor(stream[:max_len], dtype=torch.long)\n",
    "        label_processed = torch.tensor(label, dtype=torch.int64)\n",
    "        stream_list.append(stream_processed)\n",
    "        label_list.append(label_processed)\n",
    "    stream_list = pad_sequence(stream_list, batch_first=True, padding_value=0)\n",
    "\n",
    "    return stream_list, torch.stack(label_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 학습 함수 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "    매 epoch 별 학습을 위한 함수\n",
    "    :param nn.Module model: 학습 대상 모델\n",
    "    :param DataLoader train_dataloader: 학습 dataloader\n",
    "    :param nn.Module loss_fn: 학습에 사용할 loss 산정 함수\n",
    "    :param nn.Module optimizer: 학습 paramter 조정 함수\n",
    "    :param device: 학습 장치\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ### Epoch별 학습 함수 구축 ###\n",
    "    # 1. model train 상태 선언\n",
    "    # 2. 기록용 loss, 정답 수, 현재까지의 평가한 데이터 수 변수 선언\n",
    "    # 3. 배치 별 stream과 정답 학습장치 할당\n",
    "    # 4. 학습을 위한 optimizer 초기화\n",
    "    # 5. 모델을 이용한 예측된 정답 백터 도출\n",
    "    # 6. 실제 정답과 예측된 답의 loss 계산\n",
    "    # 7. loss에 기반한 역전파 연산 진행\n",
    "    # 8. 학습 optimizer step\n",
    "    # 9. 가장 확률이 높은 label 추출\n",
    "    # 10. batch 내 정답과 일치한 예측 수 확인\n",
    "    # 11. 현재까지 학습한 데이터 개수 더하기\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    cur_loss = 0\n",
    "    correct = 0\n",
    "    counts = 0\n",
    "\n",
    "    for idx, (stream, label) in enumerate (train_dataloader):\n",
    "        stream = stream.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(stream)\n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        output = output.argmax(dim=1)\n",
    "\n",
    "        correct += (output == label).sum().item()\n",
    "        counts += len(label)\n",
    "\n",
    "        cur_loss += loss.item()\n",
    "    print(\n",
    "        f\"Training loss {cur_loss / (idx+1) : .5f}, Traiing accuracy {correct / counts: 5f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 평가 함수 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, eval_dataloader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    :param nn.Module model: 평가에 사용할 모델\n",
    "    :param DataLoader eval_dataloader: 평가 데이터셋 dataloader\n",
    "    :param nn.Module loss_fn: loss 계산에 사용할 함수\n",
    "    :param device: 평가에 사용할 장치\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    ### 평가 함수 구축 ###\n",
    "    # 1. model 평가과정 선언\n",
    "    # 2. torch 초기화 선언\n",
    "    # 3. 정답율 확인을 위한 변수 및 loss 저장용 변수 선언\n",
    "    # 4. 평가 데이터의 stream data 및 정답 평가 장치에 할당\n",
    "    # 5. 모델을 통한 예측된 정답 도출\n",
    "    # 6. 예측된 정답과 실제 정답 사이 loss 산출\n",
    "    # 7. 일치한 정답 수 계산\n",
    "    # 8. 현재 loss 변수 저장\n",
    "    # 9. 전체 평균 loss 및 accuracy 계산\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        curr_loss = 0\n",
    "\n",
    "        for stream, label in eval_dataloader:\n",
    "            stream = stream.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(stream)\n",
    "            loss = loss_fn(output, label)\n",
    "            output = output.argmax(dim=1)\n",
    "            correct += (output == label).sum().item()\n",
    "            curr_loss += loss.item()\n",
    "\n",
    "    accuracy = correct / len(eval_dataloader.dataset)\n",
    "    loss_result = curr_loss / len(eval_dataloader)\n",
    "\n",
    "    return loss_result, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 전이학습 과정 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 전이학습 과정 구축 ###\n",
    "# 1. stream data csv의 데이터 추출\n",
    "# 2. 학습/평가를 위한 데이터 분리 (sklearn 이용)\n",
    "# 3. 학습/평가 데이터 Dataset 구성\n",
    "# 4. 학습 평가 Dataset for loop을 위한 DataLoader 구성\n",
    "# 5. 학습 장치 설정\n",
    "# 6. MalConv 모델 선언 및 학습된 weight load 및 설정\n",
    "\n",
    "\n",
    "\n",
    "stream, label = parse_data(DATA_PATH)\n",
    "\n",
    "train_stream, valid_stream, train_label, valid_label = train_test_split(\n",
    "    stream, label, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "train_dataset = StreamDataset(train_stream, train_label)\n",
    "valid_dataset = StreamDataset(valid_stream, valid_label)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: collate_fn(x),\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: collate_fn(x),\n",
    ")\n",
    "    \n",
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(f\"Current device is {device}\")\n",
    "\n",
    "model = MalConv(channels=256, window_size=512, embd_size=8)\n",
    "weight = torch.load(\"../data/malconv.checkpoint\")\n",
    "model.load_state_dict(weight[\"model_state_dict\"])\n",
    "\n",
    "# TODO\n",
    "# 7. FC layer 학습을 위한 FC layer 초기화\n",
    "# 8. loss 함수 및 optimizer 설정\n",
    "# 9. model 학습장치 할당\n",
    "# 10. 최소 loss 저장용 변수 설정\n",
    "# 11. 매 epoch 별 학습 진행\n",
    "# 12. 매 epoch 별 평가 진행\n",
    "# 13. epoch 별 loss 비교 후 최소 loss 설정 및 최소 loss 모델 저장\n",
    "\n",
    "\n",
    "model.fc_1 = nn.Linear(in_features=256, out_features=256, bias=True)\n",
    "model.fc_2 = nn.Linear(in_features=256, out_features=2, bias=True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "model.to(device)\n",
    "\n",
    "min_loss = 0\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    print(f\"### Train Epoch {epoch} ###\")\n",
    "    train_loss, train_accuracy = train(\n",
    "        model, train_dataloader, loss_fn, optimizer, device\n",
    "    )\n",
    "    val_loss, val_accuracy = eval(model, valid_dataloader, loss_fn, device)\n",
    "\n",
    "    print(\n",
    "        f\"### Epoch {epoch}, train_loss: {train_loss}, train_accuracy: {train_accuracy}, val_loss: {val_loss}, val_accuracy: {val_accuracy} ###\"\n",
    "    )\n",
    "\n",
    "    if epoch == 1:\n",
    "        print(\"Saving Initial model\")\n",
    "        min_loss = val_loss\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            \"./malconv_transfer_learning.pth\",\n",
    "        )\n",
    "    else:\n",
    "        if val_loss < min_loss:\n",
    "            print(\"Loss has been improved! Save model\")\n",
    "            min_loss = val_loss\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                \"./malconv_transfer_learning.pth\",\n",
    "            )\n",
    "print(\"Training Finish\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
